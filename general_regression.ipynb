{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "M = np.c_[np.ones(X_scaled.shape[0]), X_scaled]\n",
    "\n",
    "# ### 2. Establish Ground Truth\n",
    "\n",
    "print(f\"Shape of design matrix M: {M.shape}\")\n",
    "\n",
    "\n",
    "ctrue, _, _, _ = np.linalg.lstsq(M, y, rcond=None)\n",
    "\n",
    "f_true = M @ ctrue # should I use this or just y?\n",
    "\n",
    "\n",
    "# Simulation parameters\n",
    "nsims = 50\n",
    "max_samples = 1000\n",
    "# Start with enough samples to ensure the matrix is likely full rank\n",
    "nstart = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(samples, overhead_time, probs=None, nstart=50):\n",
    "    if probs is None:\n",
    "        # Uniform probability for random sampling\n",
    "        probs = np.ones_like(samples, dtype=float)\n",
    "\n",
    "    assert probs.size == samples.size\n",
    "\n",
    "    res = {'errs': [], 'MTMinv_norm':[], 'c_err': [], 'runtime':[]}\n",
    "    T = samples[:nstart]\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    for i in range(samples.size - nstart + 1):\n",
    "        MTM = M[T, :]\n",
    "        yT = y[T]\n",
    "        \n",
    "        probsi = np.sqrt(probs[:nstart+i] * (nstart+i))\n",
    "        \n",
    "        # Solve the least squares problem on the sampled subset\n",
    "        chat = np.linalg.lstsq(MTM / probsi.reshape(-1, 1), yT / probsi, rcond=None)[0]\n",
    "        \n",
    "        # Calculate error against the full-dataset model's predictions\n",
    "        res['errs'].append(np.linalg.norm(f_true - M @ chat))\n",
    "        res['c_err'].append(np.linalg.norm(ctrue - chat))\n",
    "        try: \n",
    "            Minv = np.linalg.pinv(MTM)\n",
    "            res['MTMinv_norm'].append(norm(Minv,ord=2))\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            res['MTMinv_norm'].append(np.nan)\n",
    "        \n",
    "        T = samples[:nstart+i+1]\n",
    "        res['runtime'].append(time.perf_counter() - start_time + overhead_time)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Sampling Simulations ---\n",
    "RESULTS_RAND = []\n",
    "print(\"\\nRunning Random Sampling Simulations...\")\n",
    "for j in range(nsims):\n",
    "    start_time = time.perf_counter()\n",
    "    rand_state = np.random.RandomState(42 + j)\n",
    "    samples = np.arange(X.shape[0])\n",
    "    rand_state.shuffle(samples)\n",
    "    samples = samples[:max_samples]\n",
    "    \n",
    "    uniform_probs_on_samples = np.ones_like(samples, dtype=float) / X.shape[0]\n",
    "    RESULTS_RAND.append(run_simulation(samples, time.perf_counter() - start_time, uniform_probs_on_samples, nstart=nstart))\n",
    "    print(f\"  Finished simulation {j+1}/{nsims}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Leverage Score Sampling Simulations ---\n",
    "start_time = time.perf_counter()\n",
    "Q, _ = np.linalg.qr(M)\n",
    "leverage_scores = np.linalg.norm(Q, axis=1)**2\n",
    "\n",
    "# Normalize to get sampling probabilities\n",
    "ls_probs = leverage_scores / leverage_scores.sum()\n",
    "\n",
    "RESULTS_LS = []\n",
    "overhead_time = time.perf_counter() - start_time\n",
    "print(\"\\nRunning Leverage Score Sampling Simulations...\")\n",
    "for j in range(nsims):\n",
    "    rand_state = np.random.RandomState(42 + j)\n",
    "    # Sample without replacement using the leverage score probabilities\n",
    "    ls_samples = rand_state.choice(X.shape[0], max_samples, p=ls_probs, replace=True)\n",
    "    ls_probs_on_samples = ls_probs[ls_samples]\n",
    "    RESULTS_LS.append(run_simulation(ls_samples, overhead_time, ls_probs_on_samples, nstart=nstart))\n",
    "    print(f\"  Finished simulation {j+1}/{nsims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_RAND[0]['runtime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimal_design_sub_mod import run_simulation_greedy_sub_mod\n",
    "# use profiling to figure out why runtime is shooting up at the end\n",
    "RESULTS_OD_SUB_MOD_A = [run_simulation_greedy_sub_mod(M, None, ctrue, nstart, max_samples, \"A\")[0]]\n",
    "RESULTS_OD_SUB_MOD_V = [run_simulation_greedy_sub_mod(M, None, ctrue, nstart, max_samples, \"V\")[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_to_plot = 1002 #max_samples\n",
    "min_to_plot = 5\n",
    "save = True\n",
    "\n",
    "keys = ['errs', 'MTMinv_norm', 'c_err', 'runtime']\n",
    "for key in keys:\n",
    "    if key == 'errs':\n",
    "        ylabel = r\"Error, $\\|\\hat{f}_{\\mathcal{T}} - f\\|_2$\" \n",
    "    # elif key == 'A_norm':\n",
    "    #     ylabel = r\"$\\|A\\|$\"\n",
    "    elif key == 'MTMinv_norm':\n",
    "        ylabel = r\"$\\|M_{\\mathcal{TM}}^\\dagger\\|$\"\n",
    "    elif key == 'c_err':\n",
    "        ylabel = r\"$\\|c - \\hat{c}\\|$\"\n",
    "    elif key == 'runtime':\n",
    "        ylabel = 'Time (s)'\n",
    "\n",
    "    savename = f\"{key.lower()}_with_od_sub_mod_ca_housing.png\"\n",
    "    fig, ax = plt.subplots()\n",
    "    for results, name in zip([RESULTS_RAND, RESULTS_LS, RESULTS_OD_SUB_MOD], ['Random', 'Lev. Score', 'OD Sub Mod']):\n",
    "        metric = np.array([res[key] for res in results])\n",
    "        if key == 'errs':\n",
    "            metric /= X.size \n",
    "        mean = metric.mean(axis=0)\n",
    "        # std = metric.std(axis=0)\n",
    "\n",
    "        l = ax.loglog(np.arange(nstart, len(mean) + nstart), mean, label=name, linestyle='-')\n",
    "\n",
    "\n",
    "        # ax.fill_between(np.arange(nstart, len(mean) + nstart), mean, mean + std, color=l[0].get_color(), alpha=0.5 )\n",
    "        # for err in errs:\n",
    "        #     ax.plot(np.arange(nstart, len(mean) + nstart), err, alpha=0.25, color=l[0].get_color())\n",
    "\n",
    "\n",
    "    ax.legend(title='Sampling Method', fontsize=11)\n",
    "    ax.set_xlabel(r\"Size of Training Set, $\\mathcal{T}$\", fontsize=15)\n",
    "    ax.set_ylabel(ylabel, fontsize=15)\n",
    "\n",
    "    ax.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(savename, dpi=250, format='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
